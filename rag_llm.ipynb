{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Name  \\\n",
      "0                    Panic disorder   \n",
      "1                  Vocal cord polyp   \n",
      "2                   Turner syndrome   \n",
      "3                    Cryptorchidism   \n",
      "4       Ethylene glycol poisoning-1   \n",
      "..                              ...   \n",
      "395  Urinary Stones (Kidney Stones)   \n",
      "396                    Osteoporosis   \n",
      "397            Rheumatoid Arthritis   \n",
      "398                 Type 1 Diabetes   \n",
      "399                 Type 2 Diabetes   \n",
      "\n",
      "                                              Symptoms  \\\n",
      "0    Palpitations, Sweating, Trembling, Shortness o...   \n",
      "1             Hoarseness, Vocal Changes, Vocal Fatigue   \n",
      "2    Short stature, Gonadal dysgenesis, Webbed neck...   \n",
      "3    Absence or undescended testicle(s), empty scro...   \n",
      "4    Nausea, vomiting, abdominal pain, General mala...   \n",
      "..                                                 ...   \n",
      "395  Severe abdominal or back pain, blood in urine,...   \n",
      "396  Fragile bones, loss of height over time, back ...   \n",
      "397  Joint pain, stiffness, swelling, fatigue, loss...   \n",
      "398  Frequent urination, Increased thirst, Weight loss   \n",
      "399  Fatigue, Increased hunger, Slow healing of wounds   \n",
      "\n",
      "                                            Treatments  \n",
      "0    Antidepressant medications, Cognitive Behavior...  \n",
      "1         Voice Rest, Speech Therapy, Surgical Removal  \n",
      "2    Growth hormone therapy, Estrogen replacement t...  \n",
      "3    Observation and monitoring (in cases of mild o...  \n",
      "4    Supportive Measures, Gastric Decontamination, ...  \n",
      "..                                                 ...  \n",
      "395  Pain management, increased fluid intake, medic...  \n",
      "396  Calcium and vitamin D supplements, regular exe...  \n",
      "397  Medications (nonsteroidal anti-inflammatory dr...  \n",
      "398  Insulin therapy, Blood sugar monitoring, Healt...  \n",
      "399  Oral medications, Insulin therapy (in some cas...  \n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20 Batch Size: 8, Transformer: distilgpt2:   0%|          | 0/40 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Training Epoch 1/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.81s/it, Training Loss=1.74] \n",
      "Validation Epoch 1/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 1.4001721143722534\n",
      "last lr [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:41<00:00,  4.03s/it, Training Loss=1.14] \n",
      "Validation Epoch 2/20: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, Validation Loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Validation Loss: 1.3118444681167603\n",
      "last lr [0.00085]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:35<00:00,  3.89s/it, Training Loss=0.7]  \n",
      "Validation Epoch 3/20: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it, Validation Loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Validation Loss: 1.3455476760864258\n",
      "last lr [0.0007224999999999999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.82s/it, Training Loss=0.654]\n",
      "Validation Epoch 4/20: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it, Validation Loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Validation Loss: 1.4000673294067383\n",
      "last lr [0.000614125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.651]\n",
      "Validation Epoch 5/20: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, Validation Loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Validation Loss: 1.4912079572677612\n",
      "last lr [0.00052200625]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.80s/it, Training Loss=0.352]\n",
      "Validation Epoch 6/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Validation Loss: 1.571752905845642\n",
      "last lr [0.00044370531249999997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.24] \n",
      "Validation Epoch 7/20: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it, Validation Loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Validation Loss: 1.7179937362670898\n",
      "last lr [0.00037714951562499996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.81s/it, Training Loss=0.278]\n",
      "Validation Epoch 8/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Validation Loss: 1.810288667678833\n",
      "last lr [0.00032057708828124994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.197]\n",
      "Validation Epoch 9/20: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, Validation Loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Validation Loss: 1.9638960361480713\n",
      "last lr [0.0002724905250390624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.80s/it, Training Loss=0.13] \n",
      "Validation Epoch 10/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Validation Loss: 1.9797369241714478\n",
      "last lr [0.00023161694628320305]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.0892]\n",
      "Validation Epoch 11/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Validation Loss: 2.0539543628692627\n",
      "last lr [0.0001968744043407226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.81s/it, Training Loss=0.0875]\n",
      "Validation Epoch 12/20: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it, Validation Loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Validation Loss: 2.087176561355591\n",
      "last lr [0.0001673432436896142]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:31<00:00,  3.80s/it, Training Loss=0.104] \n",
      "Validation Epoch 13/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Validation Loss: 2.093751907348633\n",
      "last lr [0.00014224175713617207]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.061] \n",
      "Validation Epoch 14/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Validation Loss: 2.109910488128662\n",
      "last lr [0.00012090549356574625]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.81s/it, Training Loss=0.049] \n",
      "Validation Epoch 15/20: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, Validation Loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Validation Loss: 2.130234479904175\n",
      "last lr [0.00010276966953088431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.82s/it, Training Loss=0.0539]\n",
      "Validation Epoch 16/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Validation Loss: 2.13869047164917\n",
      "last lr [8.735421910125166e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.82s/it, Training Loss=0.0403]\n",
      "Validation Epoch 17/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Validation Loss: 2.1288273334503174\n",
      "last lr [7.425108623606391e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.0545]\n",
      "Validation Epoch 18/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Validation Loss: 2.143779993057251\n",
      "last lr [6.311342330065433e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:33<00:00,  3.83s/it, Training Loss=0.046] \n",
      "Validation Epoch 19/20: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Validation Loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Validation Loss: 2.142712116241455\n",
      "last lr [5.3646409805556176e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [02:32<00:00,  3.81s/it, Training Loss=0.0942]\n",
      "Validation Epoch 20/20: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Validation Loss=1.93]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Validation Loss: 2.1489338874816895\n",
      "last lr [4.559944833472275e-05]\n",
      "Panic disorder | Palpitations, Sweating, Trembling, Shortness of breath, Fear of losing control, Dizziness | Antidepressant medications, Cognitive Behavioral Therapy, Relaxation Techniques\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "data_sample = load_dataset(\"QuyenAnhDE/Diseases_Symptoms\")\n",
    "updated_data = [{'Name': item['Name'], 'Symptoms': item['Symptoms'], 'Treatments': item['Treatments']} for item in data_sample['train']]\n",
    "df = pd.DataFrame(updated_data)\n",
    "print(df)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.labels = df.columns #устанавливаем метки столбцов\n",
    "        self.data = df.to_dict(orient='records')\n",
    "        self.tokenizer = tokenizer\n",
    "        #x = self.average_len(df)\n",
    "        self.max_length = 128 #в нашем лучае max_lenght  - средняя длина\n",
    "\n",
    "    def average_len(self,df):\n",
    "        sum_ = 0\n",
    "        for example in df[self.labels[2]]:\n",
    "          sum_ += len(example)\n",
    "        x  = 2\n",
    "        while x < sum_/len(df):\n",
    "          x = x * 2\n",
    "        return x\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][self.labels[0]]\n",
    "        y = self.data[idx][self.labels[1]]\n",
    "        z = self.data[idx][self.labels[2]]\n",
    "        text = f\"{x} | {y} | {z}\"\n",
    "\n",
    "        tokens = self.tokenizer.encode_plus(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True) \n",
    "        \n",
    "        return tokens\n",
    "\n",
    "data_sample = LanguageDataset(df, tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(data_sample))\n",
    "valid_size = len(data_sample) - train_size\n",
    "\n",
    "train_data, valid_data = random_split(data_sample, [train_size, valid_size])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True) #дополнительно перемешаем данные\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "num_epochs = 20\n",
    "batch_size = BATCH_SIZE\n",
    "model_name = 'distilgpt2'\n",
    "gpu = 0\n",
    "reshuffle_every = 6\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu',\n",
    "                                'training_loss', 'validation_loss', 'epoch_duration_sec'])\n",
    "\n",
    "def reshuffle_data(dataset):\n",
    "    train_data, test_data = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "    return train_data, test_data\n",
    "\n",
    "def train_model(model, num_epochs, train_loader, batch_size, model_name, sheduler, tokenizer, device):\n",
    "  for epoch in range(num_epochs):\n",
    "      start_time = time.time()  # Start the timer for the epoch\n",
    "      #переводим модель в режим обучения\n",
    "      # if epoch % reshuffle_every == 0:\n",
    "      #   train_data, test_data = reshuffle_data(data_sample)\n",
    "      #   train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "      #   test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "      model.train()\n",
    "      epoch_training_loss = 0\n",
    "\n",
    "      train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs} Batch Size: {batch_size}, Transformer: {model_name}\")\n",
    "\n",
    "      for batch in train_iterator:\n",
    "          optimizer.zero_grad()\n",
    "          inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "          targets = inputs.clone()\n",
    "\n",
    "          outputs = model(input_ids=inputs, labels=targets)\n",
    "\n",
    "          loss = outputs.loss\n",
    "          \n",
    "          #выполняем обратный переход\n",
    "          loss.backward()\n",
    "          #обновляем веса\n",
    "          optimizer.step()\n",
    "\n",
    "          train_iterator.set_postfix({'Training Loss': loss.item()})\n",
    "          epoch_training_loss += loss.item()\n",
    "\n",
    "      avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
    "\n",
    "      #переводим модель в режим ответов\n",
    "      model.eval()\n",
    "      \n",
    "      epoch_validation_loss = 0\n",
    "      total_loss = 0\n",
    "      valid_iterator = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n",
    "      with torch.no_grad():\n",
    "          for batch in valid_iterator:\n",
    "              inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "              targets = inputs.clone()\n",
    "              outputs = model(input_ids=inputs, labels=targets)\n",
    "              loss = outputs.loss\n",
    "              total_loss += loss\n",
    "              valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
    "              epoch_validation_loss += loss.item()\n",
    "\n",
    "      avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
    "\n",
    "      end_time = time.time()  # закончилась одна эпоха\n",
    "      epoch_duration_sec = end_time - start_time\n",
    "\n",
    "      new_row = {'transformer': model_name,\n",
    "                'batch_size': batch_size,\n",
    "                'gpu': gpu,\n",
    "                'epoch': epoch+1,\n",
    "                'training_loss': avg_epoch_training_loss,\n",
    "                'validation_loss': avg_epoch_validation_loss,\n",
    "                'epoch_duration_sec': epoch_duration_sec}  \n",
    "\n",
    "      results.loc[len(results)] = new_row\n",
    "      print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(valid_loader)}\")\n",
    "\n",
    "      print('last lr', sheduler.get_last_lr())\n",
    "      sheduler.step()\n",
    "\n",
    "sheduler  =  ExponentialLR(optimizer, gamma=0.85)\n",
    "train_model(model, num_epochs, train_loader, batch_size, model_name, sheduler, tokenizer, device)\n",
    "input_str = \"Panic disorder\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=70,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.8,\n",
    "    temperature=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
