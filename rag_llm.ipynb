{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)-learning DistilGPT-2 on the 'Diseases_Symptoms' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "\n",
    "This section imports essential libraries for data manipulation, tokenization, model training and result visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing the Dataset\n",
    "\n",
    "The dataset is loaded using the `load_dataset()` function, and only the relevant columns ('Name', 'Symptoms', 'Treatments') are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = load_dataset(\"QuyenAnhDE/Diseases_Symptoms\")\n",
    "updated_data = [{'Name': item['Name'], 'Symptoms': item['Symptoms'], 'Treatments': item['Treatments']} for item in data_sample['train']]\n",
    "df = pd.DataFrame(updated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring the Device\n",
    "\n",
    "The code checks for GPU availability using CUDA; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenizer and Model Initialization\n",
    "\n",
    "A pre-trained DistilGPT-2 tokenizer and model are loaded using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Class Definition\n",
    "\n",
    "The custom `LanguageDataset` class prepares the dataset by encoding input text into toke IDs using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.labels = df.columns #устанавливаем метки столбцов\n",
    "        self.data = df.to_dict(orient='records')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][self.labels[0]]\n",
    "        y = self.data[idx][self.labels[1]]\n",
    "        z = self.data[idx][self.labels[2]]\n",
    "        text = f\"{x} | {y} | {z}\"\n",
    "\n",
    "        tokens = self.tokenizer.encode_plus(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True) \n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the Dataset\n",
    "\n",
    "The dataset is split into training and validation sets with an 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = LanguageDataset(df, tokenizer)\n",
    "train_size = int(0.8 * len(data_sample))\n",
    "valid_size = len(data_sample) - train_size\n",
    "\n",
    "train_data, valid_data = random_split(data_sample, [train_size, valid_size])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True) #дополнительно перемешаем данные\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration\n",
    "\n",
    "Optimizer and result DataFrame are initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "batch_size = BATCH_SIZE\n",
    "model_name = 'distilgpt2'\n",
    "gpu = 0\n",
    "reshuffle_every = 6\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu',\n",
    "                                'training_loss', 'validation_loss', 'epoch_duration_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n",
    "\n",
    "- `reshuffle_data()`: Splits the dataset into training and validation sets.\n",
    "\n",
    "- `train_model()`: Handles the model training and validation loops for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle_data(dataset):\n",
    "    train_data, test_data = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "    return train_data, test_data\n",
    "\n",
    "def train_model(model, num_epochs, train_loader, batch_size, model_name, scheduler, tokenizer, device):\n",
    "  for epoch in range(num_epochs):\n",
    "      start_time = time.time()  # Start the timer for the epoch\n",
    "      if epoch % reshuffle_every == 0:\n",
    "        train_data, test_data = reshuffle_data(data_sample)\n",
    "        train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "        valid_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "      model.train() # Turn on the train-mode\n",
    "      epoch_training_loss = 0\n",
    "\n",
    "      train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs} Batch Size: {batch_size}, Transformer: {model_name}\")\n",
    "\n",
    "      for batch in train_iterator:\n",
    "          optimizer.zero_grad()\n",
    "          inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "          targets = inputs.clone()\n",
    "\n",
    "          outputs = model(input_ids=inputs, labels=targets)\n",
    "\n",
    "          loss = outputs.loss\n",
    "          \n",
    "          loss.backward() #backprop\n",
    "          optimizer.step() #weight update\n",
    "\n",
    "          train_iterator.set_postfix({'Training Loss': loss.item()})\n",
    "          epoch_training_loss += loss.item()\n",
    "\n",
    "      avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
    "      \n",
    "      model.eval() #Turn on the eval-mode\n",
    "      \n",
    "      epoch_validation_loss = 0\n",
    "      total_loss = 0\n",
    "      valid_iterator = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n",
    "      with torch.no_grad():\n",
    "          for batch in valid_iterator:\n",
    "              inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "              targets = inputs.clone()\n",
    "              outputs = model(input_ids=inputs, labels=targets)\n",
    "              loss = outputs.loss\n",
    "              total_loss += loss\n",
    "              valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
    "              epoch_validation_loss += loss.item()\n",
    "\n",
    "      avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
    "\n",
    "      end_time = time.time()  # End of the epoch\n",
    "      epoch_duration_sec = end_time - start_time\n",
    "\n",
    "      new_row = {'transformer': model_name,\n",
    "                'batch_size': batch_size,\n",
    "                'gpu': gpu,\n",
    "                'epoch': epoch+1,\n",
    "                'training_loss': avg_epoch_training_loss,\n",
    "                'validation_loss': avg_epoch_validation_loss,\n",
    "                'epoch_duration_sec': epoch_duration_sec}  \n",
    "\n",
    "      results.loc[len(results)] = new_row\n",
    "      print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(valid_loader)}\")\n",
    "\n",
    "      print('last lr', scheduler.get_last_lr())\n",
    "      scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Training the Model\n",
    "\n",
    "The training process is executed using the `train_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ExponentialLR(optimizer, gamma=0.85)\n",
    "train_model(model, num_epochs, train_loader, batch_size, model_name, scheduler, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation\n",
    "\n",
    "A sample input string is tokenized and passed to the model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Cellulitis\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=70,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.8,\n",
    "    temperature=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizing Training and Validation Loss\n",
    "\n",
    "The `results` DataFrame is used to plot the loss values for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss values from the results DataFrame\n",
    "epochs = results['epoch']\n",
    "training_loss = results['training_loss']\n",
    "validation_loss = results['validation_loss']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Loss per Epoch', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
